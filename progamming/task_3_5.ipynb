{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5\n",
    "\n",
    "\n",
    "## a)\n",
    "\n",
    "Consider \n",
    "$X = \\begin{pmatrix}\n",
    "1 & x_1\\\\\n",
    "1 & x_2 \\\\\n",
    "\\cdots & \\cdots \\\\\n",
    "1 & x_n\n",
    "\\end{pmatrix}$, $w = \\begin{pmatrix}\n",
    "w_1 \\\\\n",
    "w_2\n",
    "\\end{pmatrix}$ and $y = \\begin{pmatrix}\n",
    "y_1 \\\\\n",
    "\\cdots \\\\\n",
    "y_n\n",
    "\\end{pmatrix}$, then the formula of $Q(w)$ yields to\n",
    "`sum((X*w-y).^2)`.\n",
    "\n",
    "The calculus tool then calculates for the gradient:\n",
    "\n",
    "$\n",
    "  \\frac{\\partial f}{\\partial w} = 2\\cdot X^\\top \\cdot (X\\cdot w-y)\n",
    "$\n",
    "\n",
    "where\n",
    "  - $X$ is a matrix\n",
    "  - $w$ is a vector\n",
    "  - $y$ is a vector\n",
    "\n",
    "## b)\n",
    "\n",
    "Consider \n",
    "$p_{x} = \\begin{pmatrix}\n",
    "p_{x1}\\\\\n",
    "\\cdots \\\\\n",
    "p_{xn} \n",
    "\\end{pmatrix}$, $q_{i} = \\begin{pmatrix}\n",
    "q_{i1}\\\\\n",
    "\\cdots \\\\\n",
    "q_{in} \n",
    "\\end{pmatrix}$, $r_{xi}$ the scalar rating of $x$ on $i$, $lambda1$ the regulation factor for matrix $P$ and $lambda2$ the regulation factor for matrix $Q$. Note that the derivatives of $\\sum_x = ||p_x||^2$ and $\\sum_i = ||q_i||^2$ only keep the factors directly concerning $x, i$ when keeping $x, i$ fixed. \n",
    "\n",
    "\n",
    "So it yields $\\frac{\\partial \\sum_x = ||p_x||^2}{\\partial p_x^*} = \\frac{\\partial ||p_x^*||^2}{\\partial p_x^*}$ and $\\frac{\\partial \\sum_x = ||q_i||^2}{\\partial q_i^*} = \\frac{\\partial ||q_i^*||^2}{\\partial q_i^*}$\n",
    "\n",
    "\n",
    "Overall we get for the matrix calculation tool the formula\n",
    "`(r-q'*p)^2+lambda1*norm2(p)^2+lamdba2*norm2(q)^2`\n",
    "\n",
    "The calculus tool then calculates for the gradient with respect to q\n",
    "\n",
    "\n",
    "$\n",
    "  \\frac{\\partial f}{\\partial q} = 2\\cdot lamdba2\\cdot q-2\\cdot (r-q^\\top \\cdot p)\\cdot p\n",
    "$\n",
    "where\n",
    "  - $lambda1$ is a scalar\n",
    "  - $lamdba2$ is a scalar\n",
    "  - $p$ is a vector ($= p_x$)\n",
    "  - $q$ is a vector ($= q_i$)\n",
    "  - $r$ is a scalar ($= r_{xi}$)\n",
    "\n",
    "Quite similar we get for the derivative with respect to p:\n",
    "\n",
    "$\n",
    "  \\frac{\\partial f}{\\partial p} = 2\\cdot lambda1\\cdot p-2\\cdot (r-p^\\top \\cdot q)\\cdot q\n",
    "$\n",
    "where\n",
    "  - $lambda1$ is a scalar\n",
    "  - $lamdba2$ is a scalar\n",
    "  - $p$ is a vector ($= p_x$)\n",
    "  - $q$ is a vector ($= q_i$)\n",
    "  - $r$ is a scalar ($= r_{xi}$)\n",
    "\n",
    "\n",
    "If we compare that solution to the specified solution in the lecture, which is given as\n",
    "$ \\nabla q_{if} = \\sum_{x,i} = -2(r_{xi}-q_ip_x)p_{xf} + 2 \\cdot lambda2 \\cdot q_{if}$ we obtain the same result:\n",
    "The last part of the solution from exercise, e.g. $2 \\cdot lambda2 \\cdot q_{if}$ is exactly the same as in in vector notation as in the solution of the Matrix Calculator, e.g. $2\\cdot lamdba2\\cdot q$. For the first part the same applies: Within the brackets, the dot product of $p_x$ and $q_i$ gets subtracted from $r_{xi}$. This scalar will be multiplied to the vector $p$ or $q$ respectively, which is similary to the $f$-entry of this vector in the formula given in the lecture."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
