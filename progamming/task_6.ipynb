{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import regexp_replace,rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_spark():\n",
    "    spark = SparkSession.builder.appName(\"HelloWorld\").getOrCreate()\n",
    "    sc = spark.sparkContext\n",
    "    return spark, sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define a similarity measure for multisets called SMS:\n",
    "\n",
    "The divisor should be the union of the sets and elements which apear multiple times should appear as often as their maximum number of appearances in one of the sets. We denote this by $\\cup '$\n",
    "\n",
    "The divident should be the intersection of the two sets, but elements which appear multiple times in both sets should be present times the smaller number of appearances in either. We denote this operator by $\\cap '$\n",
    "\n",
    "Looking at an example - S1={A,A,B,B,B}, S2={A,A,B,C}:\n",
    "$S1 \\cap' S2 ={A,A,B}$\n",
    "$S1 \\cup' S2 ={A,A,B,B,B,C}$\n",
    "\n",
    "Putting it together we have:\n",
    "$SMS(S1,S2)=\\frac{|S1 \\cap' S2 |}{|S1 \\cup' S2 |}=\\frac{3}{6}$\n",
    "\n",
    "Testing for regular sets, in which case SMS should be equal to Jaccard:\n",
    "S1={1,2,4}, S2={2,4,7,8}\n",
    "$Jaccard(S1,S2) = \\frac{2}{5}$\n",
    "$SMS(S1,S2)=\\frac{|S1 \\cap' S2 |}{|S1 \\cup' S2 |}=\\frac{|{1,2}|}{|{1,2,4,7,8}|}= \\frac{2}{5}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First all the datasets are loaded. We converted the grundgesetzt to a txt-file. As well as created 9 other txt-files to test\n",
    "our process on (Movie-scripts of:Harry Potter1-7 + Shrek1-2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/04 20:38:03 WARN Utils: Your hostname, LT1081 resolves to a loopback address: 127.0.1.1; using 172.24.119.217 instead (on interface eth0)\n",
      "22/12/04 20:38:03 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/04 20:38:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark, sc = init_spark()\n",
    "\n",
    "dataframes= []\n",
    "\n",
    "dataframes.append(spark.read.text(\"data_sheet6/grundgesetz.txt\",wholetext=True))\n",
    "dataframes.append(spark.read.text(\"data_sheet6/hp1.txt\",wholetext=True))\n",
    "dataframes.append(spark.read.text(\"data_sheet6/hp2.txt\",wholetext=True))\n",
    "dataframes.append(spark.read.text(\"data_sheet6/hp3.txt\",wholetext=True))\n",
    "dataframes.append(spark.read.text(\"data_sheet6/hp4.txt\",wholetext=True))\n",
    "dataframes.append(spark.read.text(\"data_sheet6/hp5.txt\",wholetext=True))\n",
    "dataframes.append(spark.read.text(\"data_sheet6/hp6.txt\",wholetext=True))\n",
    "dataframes.append(spark.read.text(\"data_sheet6/hp7.txt\",wholetext=True))\n",
    "dataframes.append(spark.read.text(\"data_sheet6/shrek.txt\",wholetext=True))\n",
    "dataframes.append(spark.read.text(\"data_sheet6/shrek2.txt\",wholetext=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now we need to do some processing on the text. We will replace the newline and tab characters with whitespaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataFrame in dataframes:\n",
    "    # firstly we remove word seperations indicated by -\\n\n",
    "    dataFrame = dataFrame.withColumn(\"value\",regexp_replace(\"value\",\"-\\n\",\"\"))\n",
    "    #secondly we remove tabs and newlines\n",
    "    dataFrame = dataFrame.withColumn(\"value\",regexp_replace(\"value\",\"\\n\",\" \"))\n",
    "    dataFrame = dataFrame.withColumn(\"value\",regexp_replace(\"value\",\"\\t\",\"\"))\n",
    "    #lastly multiple whitespaces are removed and collapsed to a single one\n",
    "    dataFrame = dataFrame.withColumn(\"value\",regexp_replace(\"value\",\"\\\\s{2,}\",\" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After cleaning up we can start to create the shingles. For that we first define a shingling function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shingling_k(text,k):\n",
    "    tokens = list(text)\n",
    "    shingle = [tokens[i:i+k] for i in range(len(tokens) - k + 1)]\n",
    "    unique_shingles = []\n",
    "\n",
    "    for shingleList in shingle:\n",
    "        shingleText = \"\".join(str(i) for i in shingleList)\n",
    "        unique_shingles.append(shingleText)\n",
    "    return set(unique_shingles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can execute the singling functions for the different sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created shingles for set: Grundgesetz\n",
      "Amount of different 5 shingles: 26468\n",
      "Amount of different 9 shingles: 83065\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created shingles for set: HP1\n",
      "Amount of different 5 shingles: 75402\n",
      "Amount of different 9 shingles: 298854\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created shingles for set: HP2\n",
      "Amount of different 5 shingles: 78668\n",
      "Amount of different 9 shingles: 333446\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created shingles for set: HP3\n",
      "Amount of different 5 shingles: 87226\n",
      "Amount of different 9 shingles: 397422\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created shingles for set: HP4\n",
      "Amount of different 5 shingles: 112516\n",
      "Amount of different 9 shingles: 629863\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created shingles for set: HP5\n",
      "Amount of different 5 shingles: 132257\n",
      "Amount of different 9 shingles: 800445\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created shingles for set: HP6\n",
      "Amount of different 5 shingles: 108148\n",
      "Amount of different 9 shingles: 581263\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created shingles for set: HP7\n",
      "Amount of different 5 shingles: 116934\n",
      "Amount of different 9 shingles: 660338\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "Created shingles for set: Shrek\n",
      "Amount of different 5 shingles: 20400\n",
      "Amount of different 9 shingles: 44413\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "Created shingles for set: Shrek2\n",
      "Amount of different 5 shingles: 19669\n",
      "Amount of different 9 shingles: 40016\n"
     ]
    }
   ],
   "source": [
    "name=['Grundgesetz','HP1','HP2','HP3','HP4','HP5','HP6','HP7','Shrek','Shrek2']\n",
    "i=0\n",
    "for dataFrame in dataframes:\n",
    "    print(type(dataFrame))\n",
    "    set_of_shingles_5 = dataFrame.rdd.map(lambda row: shingling_k((row[0]),5))\n",
    "    set_of_shingles_9 = dataFrame.rdd.map(lambda row: shingling_k((row[0]),9))\n",
    "\n",
    "    set_5 = set_of_shingles_5.take(1)\n",
    "    set_9 = set_of_shingles_9.take(1)\n",
    "\n",
    "    size_of_set_5 = len(set_5[0])\n",
    "    size_of_set_9 = len(set_9[0])\n",
    "\n",
    "    print(\"Created shingles for set: \"+name[i])\n",
    "    print(\"Amount of different 5 shingles: \"+str(size_of_set_5))\n",
    "    print(\"Amount of different 9 shingles: \"+str(size_of_set_9))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Exercise 3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### a)\n",
    "\n",
    "$$\\begin{pmatrix}\n",
    "0 & 1 & 0 & 1\\\\\n",
    "0 & 1 & 0 & 0\\\\\n",
    "1 & 0 & 0 & 1\\\\\n",
    "0 & 0 & 1 & 0\\\\\n",
    "0 & 0 & 1 & 1\\\\\n",
    "1 & 0 & 0 & 0\\\\\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "#### Minhash for h_1(x)\n",
    "\n",
    "$$\\begin{pmatrix}\n",
    "5 & 1 & 3 & 1\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "#### Minhash for h_2(x)\n",
    "\n",
    "$$\\begin{pmatrix}\n",
    "2 & 2 & 2 & 2\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "#### Minhash for h_3(x)\n",
    "\n",
    "$$\\begin{pmatrix}\n",
    "0 & 1 & 4 & 0\n",
    "\\end{pmatrix}$$\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### b)\n",
    "\n",
    "None of those functions provides a true permutation?\n",
    "\n",
    "For h_1(x): S_2 and S_4 collide\n",
    "\n",
    "For h_2(x): all S collide\n",
    "\n",
    "For h_3(x): S_1 and S_4 collide"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### c)\n",
    "\n",
    "S_1 and S_2: Jaccard=0/4=0  Hashsim=1/3\n",
    "\n",
    "S_1 and S_3: Jaccard=0/4=0  Hashsim=1/3\n",
    "\n",
    "S_1 and S_4: Jaccard=1/4    Hashsim=2/3\n",
    "\n",
    "S_2 and S_3: Jaccard=0/4=0  Hashsim=1/3\n",
    "\n",
    "S_2 and S_4: Jaccard=1/4    Hashsim=2/3\n",
    "\n",
    "S_3 and S_4: Jaccard=1/4    Hashsim=1/3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Exercise 4"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If the Jaccard-similarity is 0 than, S1 and S2 do not share any elements. Since the minhash returns the element with the samllest hash value, they can never match up if S1 and S2 do not contain any shared items."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Exercise 5"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### a) $Jaccard_{S1,S2}=\\frac{1}{4}$\n",
    "\n",
    "### b)\n",
    "Whenever column d is the first for all 120 permutations the two colums hash to the same value. The probability that this is true for S1 is 1 and the probability that this happens is true for S2 in $\\frac{1}{4}$ of the cases. Hence out of all 120 permutations it is true for $120*\\frac{1}{4}=30$ of them."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Exercise 6"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The provider of some service with login needs a way to store passwords. However, it is never a good idea to store password in plain text, as else a intruder can just read out all clear text passwords. Therefore, it is common practice to hash the\n",
    "password a user selected, which will make an intrusion less harmfull. But if the hashing technique used is a quite common one\n",
    "a set of hashes can still allow you to extract the passwords. To do so one can find identical hashes and work backwords from there. Secondly one could use a so-called rainbow table, which includes a very high number of precomputed hashes for some set of passwords and just find matching ones. This way one does not have to compute all the hashes especially if the hash-technique is quite common. Lastly to truely secure a hash a provider can add a salt, which is some random long string of characters, which is added to each password before hashing. This makes rainbow tables useless and forces the hacker to truely brut-force his way through the hash."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
