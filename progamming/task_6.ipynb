{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import regexp_replace,rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_spark():\n",
    "    spark = SparkSession.builder.appName(\"HelloWorld\").getOrCreate()\n",
    "    sc = spark.sparkContext\n",
    "    return spark, sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First all the datasets are loaded. We converted the grundgesetzt to a txt-file. As well as created 9 other txt-files to test\n",
    "our process on (Movie-scripts of:Harry Potter1-7 + Shrek1-2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/02 16:20:50 WARN Utils: Your hostname, jakob-ThinkPad-E15-Gen-4 resolves to a loopback address: 127.0.1.1; using 192.168.111.224 instead (on interface wlp3s0)\n",
      "22/12/02 16:20:50 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/02 16:20:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark, sc = init_spark()\n",
    "\n",
    "dataframes= []\n",
    "\n",
    "dataframes.append(spark.read.text(\"data_sheet6/grundgesetz.txt\",wholetext=True))\n",
    "dataframes.append(spark.read.text(\"data_sheet6/hp1.txt\",wholetext=True))\n",
    "dataframes.append(spark.read.text(\"data_sheet6/hp2.txt\",wholetext=True))\n",
    "dataframes.append(spark.read.text(\"data_sheet6/hp3.txt\",wholetext=True))\n",
    "dataframes.append(spark.read.text(\"data_sheet6/hp4.txt\",wholetext=True))\n",
    "dataframes.append(spark.read.text(\"data_sheet6/hp5.txt\",wholetext=True))\n",
    "dataframes.append(spark.read.text(\"data_sheet6/hp6.txt\",wholetext=True))\n",
    "dataframes.append(spark.read.text(\"data_sheet6/hp7.txt\",wholetext=True))\n",
    "dataframes.append(spark.read.text(\"data_sheet6/shrek.txt\",wholetext=True))\n",
    "dataframes.append(spark.read.text(\"data_sheet6/shrek2.txt\",wholetext=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now we need to do some processing on the text. We will replace the newline and tab characters with whitespaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataFrame in dataframes:\n",
    "    # firstly we remove word seperations indicated by -\\n\n",
    "    dataFrame = dataFrame.withColumn(\"value\",regexp_replace(\"value\",\"-\\n\",\"\"))\n",
    "    #secondly we remove tabs and newlines\n",
    "    dataFrame = dataFrame.withColumn(\"value\",regexp_replace(\"value\",\"\\n\",\" \"))\n",
    "    dataFrame = dataFrame.withColumn(\"value\",regexp_replace(\"value\",\"\\t\",\"\"))\n",
    "    #lastly multiple whitespaces are removed and collapsed to a single one\n",
    "    dataFrame = dataFrame.withColumn(\"value\",regexp_replace(\"value\",\"\\\\s{2,}\",\" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After cleaning up we can start to create the shingles. For that we first define a shingling function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shingling_k(text,k):\n",
    "    tokens = list(text)\n",
    "    shingle = [tokens[i:i+k] for i in range(len(tokens) - k + 1)]\n",
    "    unique_shingles = []\n",
    "\n",
    "    for shingleList in shingle:\n",
    "        shingleText = \"\".join(str(i) for i in shingleList)\n",
    "        unique_shingles.append(shingleText)\n",
    "    return set(unique_shingles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can execute the singling functions for the different sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "Created shingles for set: Grundgesetz\n",
      "Amount of different 5 shingles: 26470\n",
      "Amount of different 9 shingles: 83285\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created shingles for set: HP1\n",
      "Amount of different 5 shingles: 79295\n",
      "Amount of different 9 shingles: 299736\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created shingles for set: HP2\n",
      "Amount of different 5 shingles: 82484\n",
      "Amount of different 9 shingles: 338217\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created shingles for set: HP3\n",
      "Amount of different 5 shingles: 91605\n",
      "Amount of different 9 shingles: 404934\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created shingles for set: HP4\n",
      "Amount of different 5 shingles: 118515\n",
      "Amount of different 9 shingles: 645355\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created shingles for set: HP5\n",
      "Amount of different 5 shingles: 139262\n",
      "Amount of different 9 shingles: 823049\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created shingles for set: HP6\n",
      "Amount of different 5 shingles: 113759\n",
      "Amount of different 9 shingles: 594238\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created shingles for set: HP7\n",
      "Amount of different 5 shingles: 122966\n",
      "Amount of different 9 shingles: 677162\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "Created shingles for set: Shrek\n",
      "Amount of different 5 shingles: 21090\n",
      "Amount of different 9 shingles: 44410\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "Created shingles for set: Shrek2\n",
      "Amount of different 5 shingles: 20310\n",
      "Amount of different 9 shingles: 39913\n"
     ]
    }
   ],
   "source": [
    "name=['Grundgesetz','HP1','HP2','HP3','HP4','HP5','HP6','HP7','Shrek','Shrek2']\n",
    "i=0\n",
    "for dataFrame in dataframes:\n",
    "    print(type(dataFrame))\n",
    "    set_of_shingles_5 = dataFrame.rdd.map(lambda row: shingling_k((row[0]),5))\n",
    "    set_of_shingles_9 = dataFrame.rdd.map(lambda row: shingling_k((row[0]),9))\n",
    "\n",
    "    set_5 = set_of_shingles_5.take(1)\n",
    "    set_9 = set_of_shingles_9.take(1)\n",
    "\n",
    "    size_of_set_5 = len(set_5[0])\n",
    "    size_of_set_9 = len(set_9[0])\n",
    "\n",
    "    print(\"Created shingles for set: \"+name[i])\n",
    "    print(\"Amount of different 5 shingles: \"+str(size_of_set_5))\n",
    "    print(\"Amount of different 9 shingles: \"+str(size_of_set_9))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Exercise 3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### a)\n",
    "\n",
    "$$\\begin{pmatrix}\n",
    "0 & 1 & 0 & 1\\\\\n",
    "0 & 1 & 0 & 0\\\\\n",
    "1 & 0 & 0 & 1\\\\\n",
    "0 & 0 & 1 & 0\\\\\n",
    "0 & 0 & 1 & 1\\\\\n",
    "1 & 0 & 0 & 0\\\\\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "#### Minhash for h_1(x)\n",
    "\n",
    "$$\\begin{pmatrix}\n",
    "5 & 1 & 3 & 1\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "#### Minhash for h_2(x)\n",
    "\n",
    "$$\\begin{pmatrix}\n",
    "2 & 2 & 2 & 2\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "#### Minhash for h_3(x)\n",
    "\n",
    "$$\\begin{pmatrix}\n",
    "0 & 1 & 4 & 0\n",
    "\\end{pmatrix}$$\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### b)\n",
    "\n",
    "None of those functions provides a true permutation?\n",
    "\n",
    "For h_1(x): S_2 and S_4 collide\n",
    "\n",
    "For h_2(x): all S collide\n",
    "\n",
    "For h_3(x): S_1 and S_4 collide"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### c)\n",
    "\n",
    "S_1 and S_2: Jaccard=0/4=0  Hashsim=1/3\n",
    "\n",
    "S_1 and S_3: Jaccard=0/4=0  Hashsim=1/3\n",
    "\n",
    "S_1 and S_4: Jaccard=1/4    Hashsim=2/3\n",
    "\n",
    "S_2 and S_3: Jaccard=0/4=0  Hashsim=1/3\n",
    "\n",
    "S_2 and S_4: Jaccard=1/4    Hashsim=2/3\n",
    "\n",
    "S_3 and S_4: Jaccard=1/4    Hashsim=1/3"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
